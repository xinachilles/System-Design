# zookeeper 2

Created: 2020-06-12 15:08:03 -0600

Modified: 2020-11-28 17:35:51 -0600

---

[summary of zookeeper]{.mark}



zookeepers based on raft and so we can think of it as being and indeed it is fault tolerant

and does the right thing with respect to partitions it has this sort of

performance enhancement by which reads can be processed at any replica and therefore the reads can be stale





Zookeeper does guarantee that every replicas process the stream of writes in order

one at a time with all replicas executing the writes in the same order.



so that the replicas advance sort of in their states of all than exactly the same way , all of the operation reads and writes produced by a generated by a single client or processed by the system also in order both in the order that the client issued them in and successive operations from a given client always see the same state or later in the right stream as the

previous read operation right any operation from that client

(guarantee for partial client, what its read is what is write, if client issue a write operation then immediate a read operation, what this client read is just the value its write to the system)



[why use zookeeper]{.mark}



people use is just to publish just configuration information for other servers to use like for

example the IP address of the current master for some set of workers this is

just config configuration information



another classic use of zookeepers to elect a master you know if we want to have a when the old master fails we need to have everyone agree on who the new master is and only elect one master even if there's partitions you can elect a master using zookeeper



if whatever master you elect needs to keep some state, it needs to keep it up-to-date like maybe you know informations, such as who the primary is for a given chunk of data like you'd want in GFS the master can store its state in zookeeper, it knows new keepers

not going to lose it if the master crashes and we elect a new master to replace it that new master can just read the old master state right out of zookeeper and rely on it actually being there



other things you might imagine maybe you know MapReduce like systems, workers could register themselves by creating little files and zookeeper and again

with systems like MapReduce you can imagine the master telling the workers what to do by writing things in, zookeeper like writing lists of work in zookeeper and then worker sort of take those work items one by one out of zookeeper and delete them as they

complete them but people use zookeeper for all these things







how people use zookeeper and in generally, you would if you're running some big data center and you run all kinds of stuff in your data center you know web servers storage systems ,MapReduce, who knows what you might fire up a zookeeper one zookeeper cluster because this general purpose can be used for lots of things so you know [five or seven zookeeper replicas and then as you deploy various services you would design the services to store some of the critical state in your one zookeeper cluster]{.mark}



increase the number ... 课件



[mini transaction]{.mark}



(not atomic , in the sort of zookeeper world where get() can return stale data,

so if you read a stale version of the current counter and add one to it you're now writing the wrong value



you know if values 11 but you're get returns a stale value of 10 you add 1 to that and put 11 that's a mistake because we really should have been putting 12 ,so zookeeper has this additional problem that we have to worry about that that gets don't return the latest data



)







while true:

x, v := getData("f")

if setData(x + 1, version=v):

break





our set data got to the leader , some other clients set data and some other client is trying to increment their set data got there before us our version number will no longer be fresh, in either those cases this set data will fail and we'll get an error response back it won't break out of the loop and we'll go back and try again and hopefully we'll succeed this time







(question is could this it's a while loop or we guaranteed is ever going to finish





have a thousand clients all trying to do increments, ~~the risk is that maybe none of them will succeed or something~~ I think one of them will succeed because. I think one of the most succeed because you know the first one that gets its set data into the leader will succeed and the rest will all fail because their version numbers are all too low and then the next 999 will put and get data's in and one of them will succeed so it all have a sort of N squared complexity to get through all of the all other clients

which is very damaging but it will finish eventually





and so if you thought you were gonna have a lot of clients you would use a different strategy here this

)



[memory]{.mark}



all zookeep data should store in the memory. If they fit in memory it's no problem if they don't fit memory it's a disaster so yeah when



you're using zookeeper you have to keep in mind that it's yeah it's great for

100 megabytes of stuff and probably terrible for 100 gigabytes of stuff so that's why people think of it as storing configuration information rather than their we old data of your big website





this example okay this is an example of a what many people call a mini transaction all right it's transactional in a sense that wow there's you know a lot of funny stuff happening here the effect is that once it all succeeds we have achieved an atomic read-modify-write of the counter right ,the difficulty here is that it's not atomic , [the reading the write the read the modifying the right are not atomic.]{.mark}



for the atomic read-modify write, we managed to read increment and write without anything else intervening. we managed to do these two steps atomically and you know this is not because this isn't a full database transaction like real databases allow fully general transactions where you can say start transaction and then read or write anything you like maybe thousands of different data items whatever who knows what and then say end transaction and the database will cleverly commit the whole thing as an atomic transaction



[Lock]{.mark}



acquire():
while true:
if create("lf", ephemeral=true), success
if exists("lf", watch=true)
wait for notification

release(): (voluntarily or session timeout)
delete("lf")







steps one we try to create we have a lock file and we try to create the lock file now again some file with a ephemeral set to true and so if that succeeds then or not we've acquired the lock.



The second step that doesn't succeed then we want to wait for whoever did acquire the lock. what if this isn't true that means the lock file already exists, I mean somebody else has acquired the lock and so we want to wait for them to release the lock and they're gonna release the lock by deleting this file so we're gonna watch ,we're gonna call exists and watching is true



now it turns out that um okay and if the file still exists, right which we expect it to because after all they didn't exist, presumably would have returned here so if it exists we want to wait for the notification we're waiting for this watch notification call this



three step for go to what so the usual deal is you know we call create



you know maybe we win if it fails we wait for whoever owns a lock to release it, we get the watch notification when the file is deleted, at that point this wait finishes and we go back to step 1 and try to recreate the file hopefully we will get the file this time







if one clients activities with our four steps so one we know for sure we know of already if another client calls create at the same time then the zookeeper leader is going to process those two to create RPC one at a time in some order, so either my create will be executed first or the other clients create will be executed first. my executed first, i'm going to get a true back in return and acquire the lock and the other client is guaranteed to get a false return and if there are RPC processed first they'll get the true return and i'm guaranteed to get the false return and in either case the file will be created so we're okay if we have simultaneous executions of one



the exists call is guaranteed to be executed between two log entries in the write stream,





~~it's quite important that the writes are sequenced and that reads happen at definite points between write~~





[lock without herb effect or scalable lock]{.mark}



Example: Locks without Herd Effect

(look at pseudo-code in paper, Section 2.4, page 6)

1. create a "sequential" file

2. list files

3. if no lower-numbered, lock is acquired!

4. if exists(next-lower-numbered, watch=true)

5. wait for event...

6. goto 2



Q: could a lower-numbered file be created between steps 2 and 3?

Q: can watch fire before it is the client's turn?

A: yes

lock-10 <- current lock holder

lock-11 <- next one

lock-12 <- my request



if client that created lock-11 dies before it gets the lock, the
watch will fire but it isn't my turn yet.



first have to acquire the lock



first step is create a sequential file ( just one file but with the prefix number) and so yeah we give it a prefix name( but what it actually creates is you know if this is the 27th file sequential file) created with prefix F, (maybe we get F27 or something )and in the sequenced writes that zookeeper is it's working through successive, creates get ascending, guaranteed ascending never descending, always ascending sequence numbers when you create a sequential file, there was an operation I left off from the list, it turns out you can get a list of files, you can get a list of files underneath



you give the name of Z node , that's actually a directory with files in it you can get a list of all the files that are currently in that directory, so we're gonna list the files let's start with F

that you know maybe list f star, we get some list back ,



we create a file with the system allocated us a number here, we can look at that number if there's no lower numbered file in this list then we win and we get the lock ,so if our sequential file is the lowest number file with that name prefix, we win





what's going on is that these sequentially numbered files are setting up the order in which the lock is going to be granted to the different clients,



so if we're not the winner of the lock, what we need to do is wait for the client who created the previously numbered file to release to acquire and then release the lock and we're going to release the lock,



the convention for releasing the locking in this system is for remove the file to remove your sequential file so we want to wait for the previously numbered sequential file to be deleted and then it's our turn and we get the lock







[why do you need to list the files again that's a good question]{.mark}





there's a guarantee of the sequential file creation is that once filed 27 is created no file with a lower number will ever subsequently be created, so why we need list the file again.



the answer to the question is whoever was the next lowered person might have either acquired him at least the lock before we noticed or have died.



if we're 27th in line ( he is only waiting for the number 26) ,number 26 may have died before getting the lock if number 26 dies the system automatically deletes their ephemeral files and so if that happened now we need to wait for number 25, that is the next you know



it if all files you know 2 through 27 and we're 27, if they're all they are and they're all waiting there's a lock ( waiting for its next lower number, 27 is waiting for the 26), if the one before is dies ,before getting the lock, now we need to wait for the next lower number file(25 in this case), because the next lower one is has gone away so



that's why we have to go back and relist the files in case our predecessor in the list of waiting clients turned out to die



if this exists() fails that means one of two things either my predecessor held the lock and is

released it and deleted their file or my predecessor didn't hold the lock ,they exited and zookeeper deleted their file because it was an ephemeral file





[How does this not suffer from the herd effect]{.mark}





suppose we have a thousand clients waiting and currently client made through, the

client #500 holds the lock, every client waiting ,every client is sitting here waiting for an event, but only the client that created file #501 he's waiting for the vision of file 500,



so everybody's waiting for the next lower number, so 500 is waiting for 499,but everybody everybody's waiting for just one file, when I release the lock there's only one other client ,the next higher numbered client that's waiting for my file ,so when I release

the lock one client gets a notification, one client goes back and lists the files, one client and one client now has the lock.



so the sort of expense(cost), you know no matter how many clients that are, the expense(cost) of one of each release and acquire is a constant number of RPCs .where's the expense of a release and acquire here is that every single waiting client, is notified and every single one of them sends a write request than the create request into zookeeper





Zookeeper : clever ideas for high performance by reading from any replica but the they sacrifice a bit of consistency

learn:



lock, herd effect

write from leader and read from replica







![](../media/MIT-zookeeper-2-image1.png){width="0.22916666666666666in" height="0.5833333333333334in"}

